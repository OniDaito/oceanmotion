#!/usr/bin/env python
"""
     ___                   _  _      _  o
     )) ) __  __ ___  _ _  )\/,) __  )L _  __  _ _
    ((_( ((_ (('((_( ((\( ((`(( ((_)(( (( ((_)((\(

Train the OceanMotion network. The idea is to pull in many
snippets of sonar footage, as 3D numpy arrays, then train the
U-Net for a set nuber of epochs.

An example run may look like this:

python train.py -i ./datasets/2024_03_05 -o ./runs/2024_03_06 -d cuda -e 20 -p -l 0.004 -a

This assumes you have a virtualenv of some kind up and running.

Datasets are generated by the program Crabseal which is found in another repository.
"""

from __future__ import annotations

__all__ = ["main"]
__version__ = "0.9.0"
__author__ = "Benjamin Blundell <bjb8@st-andrews.ac.uk>"

import torch
from data.generate import generate_sets_presplit
from model.test import test_model
from model.validate import validate
import wandb
import importlib
import os
import sys
import git
import logging
from torch.optim.lr_scheduler import ReduceLROnPlateau
from model.model import count_parameters
from data.dataset_presplit import MoveDatasetPreSplit
from model.loss import OceanLoss
import torch.optim as optim


def train(
    args,
    model,
    train_loader: MoveDatasetPreSplit,
    test_loader: MoveDatasetPreSplit,
    loss_func: OceanLoss,
    optimiser,
    epoch: int,
    device: str,
):
    """Train the model.

    Args:
        args (): the program arguments.
        model (): the model we are using.
        train_loader (MoveDatasetPreSplit): the training set.
        test_loader (MoveDatasetPreSplit): the test set.
        loss_func (OceanLoss): the loss function.
        optimiser (): the current optimiser from pytorch, likely Adam.
        epoch (int): the current epoch.
        device (str): the device we are on.

    """
    model.train()
    loss = None
    step = epoch * train_loader.__len__()

    # Loop through the train loader
    for batch_idx, (data, target) in enumerate(train_loader):
        data, targets = data.to(device), target.to(device)
        optimiser.zero_grad()
        preds = model(data)

        # Find the loss between prediction and target.
        loss = loss_func(preds, targets)

        if args.wandb:
            wandb.log({"training loss": loss.item()})

        # Print the prediction and loss to make sure it's going alright.
        if batch_idx % args.log_interval == 0:
            logging.info(
                "Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}".format(
                    epoch,
                    batch_idx * len(data),
                    len(train_loader.dataset),
                    100.0 * batch_idx / len(train_loader),
                    loss.item(),
                )
            )

        # Run through the test set and plot / save
        if batch_idx % args.test_interval == 0:
            test_model(
                model, test_loader, loss_func, device, args.wandb, confidence=0.7
            )
            model.train()

        # Back-prop and optimiser step
        loss.backward()
        optimiser.step()
        step += 1

    return loss  # Returned each epoch so we can save it.


def main(args):
    """Read the arguments, initialise all the various subsystems
    then run the training / test loop."""
    train_loader = None
    test_loader = None
    val_loader = None

    format = "%(asctime)s - %(levelname)s : %(message)s"
    logging.basicConfig(
        filename=os.path.join(args.out_path, "train.log"),
        format=format,
        level=logging.INFO,
        datefmt="%Y-%m-%d %H:%M:%S",
    )

    formatter = logging.Formatter(format)
    handler = logging.StreamHandler(sys.stdout)
    handler.setFormatter(formatter)
    logging.getLogger().addHandler(handler)

    repo = git.Repo(search_parent_directories=True)
    sha = repo.head.object.hexsha

    logging.info("Git Commit Sha: %s", sha)
    logging.info("Training Options %s", str(args))

    # We ignore multiclass
    num_classes = 1

    # Load the data and split into train and test
    # preselected train/test/val from crabseal.
    (
        train_loader,
        test_loader,
        val_loader,
        _,
        _,
        _,
    ) = generate_sets_presplit(args, num_classes)

    # Make the model. Do so by reading the model_class parameter
    ModelMoveType = getattr(importlib.import_module("model.model"), args.model_class)
    model = ModelMoveType(1, num_classes)

    device = torch.device("cpu" if not torch.cuda.is_available() else args.device)
    model.to(device)

    # Count the number of trainable parameters
    num_parameters = count_parameters(model)

    # Print the result
    print("Number of trainable parameters: ", num_parameters)

    if args.wandb:
        wandb.init(
            project="oceanmotion",
            config={"dir": args.out_path},
            dir=args.out_path,
            resume=args.checkpoint,
        )
        wandb.config.update(args)
        wandb.watch(model)

    loss_func = OceanLoss()
    optimiser = optim.Adam(model.parameters(), lr=args.learning_rate)
    scheduler = ReduceLROnPlateau(optimiser, "min")
    start_epoch = 1

    # Load from a checkpoint if that option is selected
    if args.checkpoint:
        cp_path = os.path.join(args.out_path, "checkpoint.pt")
        assert os.path.exists(cp_path)
        checkpoint = torch.load(cp_path)
        model.load_state_dict(checkpoint["model_state_dict"])
        optimiser.load_state_dict(checkpoint["optimiser_state_dict"])
        start_epoch = (
            checkpoint["epoch"] + 1
        )  # as we saved at the end of the previous epoch
        # loss = checkpoint['loss']
        model.train()

    # Now perform the training and do a test after each epoch.
    for epoch in range(start_epoch, args.epochs + 1):
        train_loss = train(
            args,
            model,
            train_loader,
            test_loader,
            loss_func,
            optimiser,
            epoch,
            device,
        )

        if args.schedule:
            val_loss = validate(model, val_loader, loss_func, device)
            scheduler.step(val_loss)

        # Save after each epoch
        # Epoch interval is also the save interval for now
        torch.save(
            {
                "epoch": epoch,  # End of current epoch
                "model_state_dict": model.state_dict(),
                "optimiser_state_dict": optimiser.state_dict(),
                "loss": train_loss,
            },
            os.path.join(args.out_path, "checkpoint.pt"),
        )

    torch.save(model.state_dict(), os.path.join(args.out_path, "model.pt"))
    logging.info("Saved PyTorch Model State to model.pt")


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Train our neural network")
    parser.add_argument("-i", "--data_path", default="./data")
    parser.add_argument("-o", "--out_path", default="./out")
    parser.add_argument("-d", "--device", default="cuda")
    parser.add_argument("-e", "--epochs", type=int, default=10)
    parser.add_argument("-b", "--batch_size", type=int, default=2)
    parser.add_argument("--seed", type=int, default=9001)
    parser.add_argument(
        "-t",
        "--model_class",
        default="UNetTRed",
        help="The model class to load (default: UNetTRed)",
    )
    parser.add_argument(
        "-l",
        "--learning_rate",
        type=float,
        default=1e-4,
        help="The learning rate (default: 1e-4)",
    )
    parser.add_argument(
        "--log_interval",
        type=int,
        default=10,
        help="how many batches to wait before logging training status",
    )
    parser.add_argument(
        "--test_interval",
        type=int,
        default=100,
        help="how many batches to wait before logging test status",
    )
    parser.add_argument(
        "-a",
        "--wandb",
        action="store_true",
        help="Do we log to wandb (default: False)?",
    )
    parser.add_argument(
        "-c",
        "--checkpoint",
        action="store_true",
        default=False,
        help="Start training from the last checkpoint (default: false)",
    )
    parser.add_argument(
        "--schedule",
        action="store_true",
        default=False,
        help="Run the validation scheduler (default: false)",
    )

    args = parser.parse_args()
    main(args)
